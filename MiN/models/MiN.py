import math
import random
import numpy as np
from tqdm import tqdm
import torch
from torch import optim
from torch.nn import functional as F
from torch.utils.data import DataLoader
import copy
import gc 
import os

from utils.inc_net import MiNbaseNet
from torch.utils.data import WeightedRandomSampler
from utils.toolkit import tensor2numpy, count_parameters
from data_process.data_manger import DataManger
from utils.training_tool import get_optimizer, get_scheduler
from utils.toolkit import calculate_class_metrics, calculate_task_metrics

# Import Mixed Precision
from torch.amp import autocast, GradScaler

EPSILON = 1e-8

class MinNet(object):
    def __init__(self, args, loger):
        super().__init__()
        self.args = args
        self.logger = loger
        self._network = MiNbaseNet(args)
        self.device = args['device']
        self.num_workers = args["num_workers"]

        self.init_epochs = args["init_epochs"]
        self.init_lr = args["init_lr"]
        self.init_weight_decay = args["init_weight_decay"]
        self.init_batch_size = args["init_batch_size"]

        self.lr = args["lr"]
        self.batch_size = args["batch_size"]
        self.weight_decay = args["weight_decay"]
        self.epochs = args["epochs"]

        self.init_class = args["init_class"]
        self.increment = args["increment"]

        self.buffer_size = args["buffer_size"]
        self.buffer_batch = args["buffer_batch"]
        self.gamma = args['gamma']
        self.fit_epoch = args["fit_epochs"]

        self.known_class = 0
        self.cur_task = -1
        self.total_acc = []
        self.class_acc = []
        self.task_acc = []
        
        # Scaler cho Mixed Precision
        self.scaler = GradScaler('cuda')

    def _clear_gpu(self):
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def after_train(self, data_manger):
        if self.cur_task == 0:
            self.known_class = self.init_class
        else:
            self.known_class += self.increment

        _, test_list, _ = data_manger.get_task_list(self.cur_task)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)
        test_loader = DataLoader(test_set, batch_size=self.init_batch_size, shuffle=False,
                                 num_workers=self.num_workers)
        eval_res = self.eval_task(test_loader)
        self.total_acc.append(round(float(eval_res['all_class_accy']*100.), 2))
        self.logger.info('total acc: {}'.format(self.total_acc))
        self.logger.info('avg_acc: {:.2f}'.format(np.mean(self.total_acc)))
        self.logger.info('task_confusion_metrix:\n{}'.format(eval_res['task_confusion']))
        print('total acc: {}'.format(self.total_acc))
        print('avg_acc: {:.2f}'.format(np.mean(self.total_acc)))
        
        del test_set

    def save_check_point(self, path_name):
        torch.save(self._network.state_dict(), path_name)

    def compute_test_acc(self, test_loader):
        model = self._network.eval()
        correct, total = 0, 0
        device = self.device
        with torch.no_grad(), autocast('cuda'):
            for i, (_, inputs, targets) in enumerate(test_loader):
                inputs = inputs.to(device)
                outputs = model(inputs)
                logits = outputs["logits"]
                predicts = torch.max(logits, dim=1)[1]
                correct += (predicts.cpu() == targets).sum()
                total += len(targets)
        return np.around(tensor2numpy(correct) * 100 / total, decimals=2)

    @staticmethod
    def cat2order(targets, datamanger):
        for i in range(len(targets)):
            targets[i] = datamanger.map_cat2order(targets[i])
        return targets

    def init_train(self, data_manger):
        self.cur_task += 1
        train_list, test_list, train_list_name = data_manger.get_task_list(0)
        self.logger.info("task_list: {}".format(train_list_name))
        self.logger.info("task_order: {}".format(train_list))

        train_set = data_manger.get_task_data(source="train", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)

        train_loader = DataLoader(train_set, batch_size=self.init_batch_size, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.init_batch_size, shuffle=False,
                                 num_workers=self.num_workers)

        self.test_loader = test_loader

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = True

        self._network.update_fc(self.init_class)
        self._network.update_noise()
        
        self._clear_gpu()
        
        self.run(train_loader)
        self._network.collect_projections(mode='threshold', val=0.95)
        
        
        self._clear_gpu()
        
        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)
        self.fit_fc(train_loader, test_loader)

        train_set = data_manger.get_task_data(source="train_no_aug", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False

        self.re_fit(train_loader, test_loader)
        #self.check_rls_quality()
        del train_set, test_set
        self._clear_gpu()

    def increment_train(self, data_manger):
        self.cur_task += 1
        train_list, test_list, train_list_name = data_manger.get_task_list(self.cur_task)
        self.logger.info("task_list: {}".format(train_list_name))
        self.logger.info("task_order: {}".format(train_list))

        train_set = data_manger.get_task_data(source="train", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)

        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)
        self.test_loader = test_loader

        # [FIX 1: QUAN TRá»ŒNG] Pháº£i update FC (má»Ÿ rá»™ng class) TRÆ¯á»šC KHI fit
        # Äá»ƒ fit_fc biáº¿t Ä‘Æ°á»£c Ä‘Ãºng sá»‘ lÆ°á»£ng class má»›i
        self._network.update_fc(self.increment)
        
        # Update Noise Generator cho task má»›i
        self._network.update_noise()

        # [STEP 1] Analytic Learning (RLS)
        # Fit trÃªn dá»¯ liá»‡u task má»›i (Ä‘á»“ng thá»i tÃ­ch lÅ©y vÃ o bá»™ nhá»› A_global, B_global)
        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False
        
        self.fit_fc(train_loader, test_loader)

        # [STEP 2] Training Noise (SGD)
        # Táº¡o láº¡i loader vá»›i batch_size nhá» hÆ¡n cho viá»‡c train noise
        train_loader_sgd = DataLoader(train_set, batch_size=self.batch_size, shuffle=True,
                                        num_workers=self.num_workers)
        
        self._clear_gpu()
        self.run(train_loader_sgd)
        
        # Thu tháº­p GPM Projection sau khi train xong noise
        self._network.collect_projections(mode='threshold', val=0.95)
        self._clear_gpu()

        del train_set

        # [STEP 3] Re-Fit Analytic Classifier (Final Polish)
        # DÃ¹ng táº­p train khÃ´ng augmentation Ä‘á»ƒ chá»‘t háº¡ classifier
        train_set_no_aug = data_manger.get_task_data(source="train_no_aug", class_list=train_list)
        train_set_no_aug.labels = self.cat2order(train_set_no_aug.labels, data_manger)
        
        train_loader_no_aug = DataLoader(train_set_no_aug, batch_size=self.buffer_batch, shuffle=True,
                                         num_workers=self.num_workers)

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False

        self.re_fit(train_loader_no_aug, test_loader)
        
        del train_set_no_aug, test_set
        self._clear_gpu()

    # def fit_fc(self, train_loader, test_loader):
    #     # [FIX 2: MEMORY ACCUMULATION]
    #     # RLS cáº§n nhá»› ma tráº­n tÆ°Æ¡ng quan (A) vÃ  (B) cá»§a cÃ¡c task cÅ©.
    #     # Náº¿u tÃ­nh láº¡i tá»« Ä‘áº§u, model sáº½ quÃªn sáº¡ch quÃ¡ khá»©.
        
    #     self._network.eval()
    #     self._network.to(self.device)
        
    #     # 1. XÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c feature
    #     with torch.no_grad():
    #         dummy_input = next(iter(train_loader))[1].to(self.device)
    #         dummy_feat = self._network.extract_feature(dummy_input)
    #         if hasattr(self._network, 'buffer'):
    #             dummy_feat = self._network.buffer(dummy_feat.float())
    #         feat_dim = dummy_feat.shape[1]
        
    #     # Láº¥y tá»•ng sá»‘ class ÄÃƒ ÄÆ¯á»¢C Má»ž Rá»˜NG
    #     num_classes = self._network.known_class
        
    #     # 2. Khá»Ÿi táº¡o Global Memory náº¿u chÆ°a cÃ³ (lÆ°u trong self cá»§a MinNet Ä‘á»ƒ persist qua cÃ¡c task)
    #     if not hasattr(self, 'A_global'):
    #         print("--> Initializing Global RLS Memory...")
    #         self.A_global = torch.zeros((feat_dim, feat_dim), device=self.device, dtype=torch.float32)
    #         self.B_global = torch.zeros((feat_dim, 0), device=self.device, dtype=torch.float32)

    #     # Má»Ÿ rá»™ng B_global náº¿u sá»‘ class tÄƒng lÃªn
    #     current_B_cols = self.B_global.shape[1]
    #     if num_classes > current_B_cols:
    #         diff = num_classes - current_B_cols
    #         expansion = torch.zeros((feat_dim, diff), device=self.device, dtype=torch.float32)
    #         self.B_global = torch.cat([self.B_global, expansion], dim=1)
            
    #     print(f"--> Accumulating Statistics for Task {self.cur_task} (Total Classes: {num_classes})...")
        
    #     # 3. TÃ­ch lÅ©y thá»‘ng kÃª (Chá»‰ cá»™ng thÃªm pháº§n cá»§a Task má»›i)
    #     # LÆ°u Ã½: A_new vÃ  B_new lÃ  thá»‘ng kÃª cá»§a RIÃŠNG dá»¯ liá»‡u hiá»‡n táº¡i
    #     A_new = torch.zeros((feat_dim, feat_dim), device=self.device, dtype=torch.float32)
    #     B_new = torch.zeros((feat_dim, num_classes), device=self.device, dtype=torch.float32)
    #     fit_epochs = self.fit_epoch 
        
    #     for epoch in range(fit_epochs):
    #         with torch.no_grad():
    #             for i, (_, inputs, targets) in enumerate(tqdm(train_loader, desc=f"Ep {epoch+1}")):
    #                 inputs, targets = inputs.to(self.device), targets.to(self.device)
    #                 if targets.dim() > 1: targets = targets.view(-1)
                    
    #                 # Forward
    #                 features = self._network.extract_feature(inputs).float()
    #                 features = self._network.buffer(features)
                    
    #                 # One-hot (Ä‘Ã£ an toÃ n vÃ¬ num_classes Ä‘Æ°á»£c update tá»« bÆ°á»›c update_fc)
    #                 targets_oh = F.one_hot(targets.long(), num_classes=num_classes).float()
                    
    #                 A_new += features.T @ features
    #                 B_new += features.T @ targets_oh
        
    #     # 4. Cá»™ng vÃ o bá»™ nhá»› toÃ n cá»¥c
    #     self.A_global += A_new
    #     self.B_global += B_new
        
    #     # 5. Giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh trÃªn bá»™ nhá»› toÃ n cá»¥c
    #     # W = (A_global + gamma * I)^-1 @ B_global
    #     print("--> Solving Global Linear System...")
    #     gamma = self.args['gamma']
    #     I = torch.eye(feat_dim, device=self.device, dtype=torch.float32)
    #     A_reg = self.A_global + gamma * I
        
    #     try:
    #         W = torch.linalg.solve(A_reg, self.B_global)
    #     except RuntimeError:
    #         W = torch.linalg.pinv(A_reg) @ self.B_global
            
    #     # 6. GÃ¡n trá»ng sá»‘
    #     if self._network.weight.shape != W.shape:
    #         self._network.weight = torch.zeros_like(W)
    #     self._network.weight.data = W
            
    #     print("--> Analytic Learning Finished.")
    #     self._clear_gpu()

    # def re_fit(self, train_loader, test_loader):
    #     # re_fit dÃ¹ng chung logic vá»›i fit_fc
    #     print(f"--> Refitting Task {self.cur_task} (No Augmentation)...")
    #     self.fit_fc(train_loader, test_loader)
       
    #Fit vÃ  refit cá»§a mmmc
    # Thay tháº¿ hÃ m fit_fc cÅ© trong class MinNet (Min.py)
    
    def fit_fc(self, train_loader, test_loader, update_archive=True):
        self._network.eval()
        self._network.to(self.device)
        
        # --- Cáº¥u hÃ¬nh Hyperparams cho MMCC ---
        # Báº¡n cÃ³ thá»ƒ Ä‘Æ°a cÃ¡i nÃ y ra file config args
        SIGMA = 1.0  # Ban Ä‘áº§u = 1.0. TÄƒng lÃªn náº¿u data quÃ¡ nhiá»…u.
        OMEGA = 0.9  # Tá»· lá»‡ trá»™n (0.4 Gaussian, 0.6 Cauchy)
        
        # 1. Láº¥y Feature Dim & Init Memory (Giá»¯ nguyÃªn nhÆ° cÅ©)
        with torch.no_grad():
            dummy_input = next(iter(train_loader))[1].to(self.device)
            dummy_feat = self._network.extract_feature(dummy_input)
            if hasattr(self._network, 'buffer'):
                dummy_feat = self._network.buffer(dummy_feat.float())
            feat_dim = dummy_feat.shape[1]

        num_classes = self._network.known_class
        
        if not hasattr(self, 'A_global'):
            print("--> Initializing MMCC-RLS Global Memory...")
            self.A_global = torch.zeros((feat_dim, feat_dim), device=self.device, dtype=torch.float32)
            self.B_global = torch.zeros((feat_dim, 0), device=self.device, dtype=torch.float32)

        if num_classes > self.B_global.shape[1]:
            diff = num_classes - self.B_global.shape[1]
            expansion = torch.zeros((feat_dim, diff), device=self.device, dtype=torch.float32)
            self.B_global = torch.cat([self.B_global, expansion], dim=1)

        # 2. ITERATIVE LEARNING (IRLS)
        # MMCC cáº§n trá»ng sá»‘ W ban Ä‘áº§u Ä‘á»ƒ tÃ­nh lá»—i. 
        # Náº¿u lÃ  epoch 0, ta dÃ¹ng W hiá»‡n táº¡i (hoáº·c khá»Ÿi táº¡o ngáº«u nhiÃªn náº¿u task 0)
        current_W = self._network.weight.data.clone()
        if current_W.shape[1] != num_classes: # Resize náº¿u class tÄƒng
             # (Logic resize W Ä‘Æ¡n giáº£n, hoáº·c Ä‘á»ƒ 0 cÅ©ng Ä‘Æ°á»£c vÃ¬ IRLS sáº½ há»™i tá»¥ nhanh)
             new_W = torch.zeros((feat_dim, num_classes), device=self.device)
             new_W[:, :current_W.shape[1]] = current_W
             current_W = new_W

        current_fit_epochs = self.fit_epoch if update_archive else 1 
        
        print(f"--> MMCC-RLS Fitting (Sigma={SIGMA}, Omega={OMEGA})...")
        
        # VÃ²ng láº·p tinh chá»‰nh trá»ng sá»‘ (The Robust Loop)
        for epoch in range(current_fit_epochs):
            # Reset Accumulators cho Epoch nÃ y
            A_epoch = torch.zeros((feat_dim, feat_dim), device=self.device, dtype=torch.float32)
            B_epoch = torch.zeros((feat_dim, num_classes), device=self.device, dtype=torch.float32)
            
            with torch.no_grad():
                for i, (_, inputs, targets) in enumerate(tqdm(train_loader, desc=f"MMCC Ep {epoch+1}", leave=False)):
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    if targets.dim() > 1: targets = targets.view(-1)
                    
                    # Forward láº¥y feature
                    features = self._network.extract_feature(inputs).float()
                    features = self._network.buffer(features)
                    
                    # One-hot Targets
                    targets_oh = F.one_hot(targets.long(), num_classes=num_classes).float()
                    
                    # [CORE MMCC]: TÃ­nh A vÃ  B cÃ³ trá»ng sá»‘ dá»±a trÃªn W cá»§a vÃ²ng láº·p trÆ°á»›c
                    A_batch, B_batch = self._network.fit_mmcc(
                        features, targets_oh, current_W, sigma=None, omega=OMEGA
                    )
                    
                    A_epoch += A_batch
                    B_epoch += B_batch
            
            # Giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh táº¡m thá»i Ä‘á»ƒ cáº­p nháº­t W cho epoch sau (Refinement)
            # W_new = (A_global + A_epoch + gamma*I)^-1 @ (B_global + B_epoch)
            # LÆ°u Ã½: Ta cá»™ng vá»›i A_global Ä‘á»ƒ táº­n dá»¥ng kiáº¿n thá»©c cÅ© giÃºp Ä‘á»‹nh hÃ¬nh lá»—i tá»‘t hÆ¡n
            
            gamma = self.args['gamma']
            I = torch.eye(feat_dim, device=self.device, dtype=torch.float32)
            
            # Náº¿u update_archive=False (Refit), ta dÃ¹ng A_global nhÆ°ng khÃ´ng sá»­a nÃ³
            # Náº¿u update_archive=True, ta chÆ°a sá»­a A_global ngay mÃ  Ä‘á»£i háº¿t loop má»›i sá»­a (hoáº·c sá»­a luÃ´n tÃ¹y chiáº¿n thuáº­t)
            # á»ž Ä‘Ã¢y: Ta dÃ¹ng W táº¡m Ä‘á»ƒ tÃ­nh lá»—i cho chÃ­nh xÃ¡c hÆ¡n á»Ÿ epoch sau
            try:
                current_W = torch.linalg.solve(self.A_global + A_epoch + gamma * I, self.B_global + B_epoch)
            except RuntimeError:
                current_W = torch.linalg.pinv(self.A_global + A_epoch + gamma * I) @ (self.B_global + B_epoch)

        # 3. Final Update to Memory & Network
        print("--> Solving Final MMCC System...")
        
        if update_archive:
            # Chá»‰ cá»™ng dá»“n thá»‘ng kÃª cá»§a Epoch CUá»I CÃ™NG (nÆ¡i trá»ng sá»‘ eta chÃ­nh xÃ¡c nháº¥t)
            # Hoáº·c trung bÃ¬nh cÃ¡c epoch. á»ž Ä‘Ã¢y láº¥y epoch cuá»‘i lÃ  chuáº©n nháº¥t theo IRLS.
            self.A_global += A_epoch
            self.B_global += B_epoch
        
        # GÃ¡n trá»ng sá»‘ Ä‘Ã£ giáº£i Ä‘Æ°á»£c vÃ o máº¡ng
        if self._network.weight.shape != current_W.shape:
            self._network.weight = torch.zeros_like(current_W)
        self._network.weight.data = current_W
        
        self._clear_gpu()
    def re_fit(self, train_loader, test_loader):
        """
        Re-fit vá»›i MMCC trÃªn dá»¯ liá»‡u sáº¡ch.
        
        Logic:
        1. Váº«n dÃ¹ng cÆ¡ cháº¿ MMCC Ä‘á»ƒ lá»c nhiá»…u (cho classifier mÆ°á»£t hÆ¡n).
        2. update_archive=False: TUYá»†T Äá»I KHÃ”NG cá»™ng dá»“n thá»‘ng kÃª vÃ o A_global.
           Ta chá»‰ mÆ°á»£n kiáº¿n thá»©c cÅ© (A_global) + dá»¯ liá»‡u sáº¡ch hiá»‡n táº¡i -> Ra W tá»‘t nháº¥t Ä‘á»ƒ test.
        """
        print(f"--> [MMCC] Re-fitting Task {self.cur_task} with Clean Data...")
        
        # Gá»i fit_fc vá»›i cháº¿ Ä‘á»™ KHÃ”NG LÆ¯U
        self.fit_fc(train_loader, test_loader, update_archive=False)



    def run(self, train_loader):
        epochs = self.init_epochs if self.cur_task == 0 else self.epochs
        lr = self.init_lr if self.cur_task == 0 else self.lr
        weight_decay = self.init_weight_decay if self.cur_task == 0 else self.weight_decay

        current_scale = 0.85 
        
        # Freeze/Unfreeze Logic
        for param in self._network.parameters(): param.requires_grad = False
        for param in self._network.normal_fc.parameters(): param.requires_grad = True
        
        if self.cur_task == 0: self._network.init_unfreeze()
        else: self._network.unfreeze_noise()
            
        params = filter(lambda p: p.requires_grad, self._network.parameters())
        optimizer = get_optimizer(self.args['optimizer_type'], params, lr, weight_decay)
        scheduler = get_scheduler(self.args['scheduler_type'], optimizer, epochs)

        prog_bar = tqdm(range(epochs))
        self._network.train()
        self._network.to(self.device)

        WARMUP_EPOCHS = 2
        max_beta = 1e-4 # [LÆ¯U Ã] Chá»‰nh láº¡i max_beta tÃ¹y Ã½ báº¡n (1e-4 hoáº·c 1e-5)
        
        for _, epoch in enumerate(prog_bar):
            losses = 0.0
            ce_losses = 0.0 # Theo dÃµi riÃªng CE
            kl_losses = 0.0 # Theo dÃµi riÃªng KL
            correct, total = 0, 0

            beta_current = max_beta * min(1.0, epoch / (epochs / 2 + 1e-6))

            for i, (_, inputs, targets) in enumerate(train_loader):
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                optimizer.zero_grad(set_to_none=True) 

                # 1. FORWARD
                with autocast('cuda'):
                    if self.cur_task > 0:
                        with torch.no_grad():
                            logits1 = self._network(inputs, new_forward=False)['logits']
                        logits2, batch_kl = self._network.forward_with_ib(inputs)
                        logits_final = logits2 + logits1
                    else:
                        logits_final, batch_kl = self._network.forward_with_ib(inputs)

                # 2. CALC LOSS
                logits_final = logits_final.float() 
                if targets.dim() > 1: targets = targets.reshape(-1)
                targets = targets.long()

                ce_loss = F.cross_entropy(logits_final, targets)
                loss = ce_loss + beta_current * batch_kl

                # 3. BACKWARD
                self.scaler.scale(loss).backward()
                
                if self.cur_task > 0 and epoch >= WARMUP_EPOCHS:
                    self.scaler.unscale_(optimizer)
                    self._network.apply_gpm_to_grads(scale=current_scale)
                
                self.scaler.step(optimizer)
                self.scaler.update()
                
                # 4. METRICS & LOGGING
                losses += loss.item()
                ce_losses += ce_loss.item()      # Cá»™ng dá»“n CE
                kl_losses += batch_kl.item()     # Cá»™ng dá»“n KL
                
                _, preds = torch.max(logits_final, dim=1)
                correct += preds.eq(targets.expand_as(preds)).cpu().sum()
                total += len(targets)
                
                del inputs, targets, loss, logits_final, batch_kl

                # In báº£ng Noise má»—i 50 batch
                if i % 50 == 0:
                     if self.cur_task > 0 or (self.cur_task == 0 and epoch == epochs - 1):
                        self.print_noise_status()

            scheduler.step()
            train_acc = 100. * correct / total

            # [HIá»‚N THá»Š CHI TIáº¾T LOSS]
            # L: Tá»•ng | CE: CrossEntropy (Dá»± Ä‘oÃ¡n) | KL: IB Loss (NÃ©n)
            info = "T {} | Ep {} | L {:.3f} (CE {:.3f} | KL {:.1f}) | Acc {:.2f}".format(
                self.cur_task, epoch + 1, 
                losses / len(train_loader), 
                ce_losses / len(train_loader),
                kl_losses / len(train_loader),
                train_acc
            )
            self.logger.info(info)
            prog_bar.set_description(info)
            
            if epoch % 5 == 0:
                self._clear_gpu()
    
    
    def print_noise_status(self):
        print("\n" + "="*85)
        print(f"{'Layer':<10} | {'Signal':<10} | {'Noise':<10} | {'SNR':<10} | {'Sigma':<10} | {'Scale':<10} | {'Status'}")
        print("-" * 85)
        
        # Láº¥y danh sÃ¡ch cÃ¡c lá»›p Noise tá»« backbone
        # LÆ°u Ã½: Cáº¥u trÃºc backbone cá»§a báº¡n cÃ³ thá»ƒ khÃ¡c, hÃ£y Ä‘áº£m báº£o path Ä‘Ãºng
        # VÃ­ dá»¥: self._network.backbone.noise_maker
        noise_layers = []
        if hasattr(self._network.backbone, 'noise_maker'):
             noise_layers = self._network.backbone.noise_maker
        
        for i, layer in enumerate(noise_layers):
            # Kiá»ƒm tra xem lá»›p Ä‘Ã³ cÃ³ biáº¿n last_debug_info khÃ´ng (Ä‘Ã£ thÃªm á»Ÿ bÆ°á»›c trÆ°á»›c)
            if not hasattr(layer, 'last_debug_info') or not layer.last_debug_info: 
                continue
            
            info = layer.last_debug_info
            
            # ÄÃ¡nh giÃ¡ tráº¡ng thÃ¡i
            snr = info['snr']
            if snr < 1.0: status = "TOXIC â˜ ï¸"       # Nhiá»…u to hÆ¡n tÃ­n hiá»‡u
            elif snr < 10.0: status = "HEAVY âš ï¸"    # Nhiá»…u náº·ng
            elif snr > 1000.0: status = "USELESS ðŸ’¤" # Nhiá»…u quÃ¡ bÃ©
            else: status = "GOOD âœ…"                # 10 < SNR < 1000
            
            print(f"L{i:<9} | {info['signal']:.4f}     | {info['noise']:.4f}     | {snr:.1f}       | {info['sigma']:.4f}     | {info['scale']:.4f}     | {status}")
        print("="*85 + "\n")
    def eval_task(self, test_loader):
        model = self._network.eval()
        pred, label = [], []
        with torch.no_grad():
            for i, (_, inputs, targets) in enumerate(test_loader):
                inputs = inputs.to(self.device)
                outputs = model(inputs)
                logits = outputs["logits"]
                predicts = torch.max(logits, dim=1)[1]
                pred.extend([int(predicts[i].cpu().numpy()) for i in range(predicts.shape[0])])
                label.extend(int(targets[i].cpu().numpy()) for i in range(targets.shape[0]))
        class_info = calculate_class_metrics(pred, label)
        task_info = calculate_task_metrics(pred, label, self.init_class, self.increment)
        return {
            "all_class_accy": class_info['all_accy'],
            "class_accy": class_info['class_accy'],
            "class_confusion": class_info['class_confusion_matrices'],
            "task_accy": task_info['all_accy'],
            "task_confusion": task_info['task_confusion_matrices'],
            "all_task_accy": task_info['task_accy'],
        }

    def get_task_prototype(self, model, train_loader):
        model = model.eval()
        model.to(self.device)
        features = []
        with torch.no_grad():
            for i, (_, inputs, targets) in enumerate(train_loader):
                inputs = inputs.to(self.device)
                with autocast('cuda'):
                    feature = model.extract_feature(inputs)
                features.append(feature.detach().cpu())
        
        all_features = torch.cat(features, dim=0)
        prototype = torch.mean(all_features, dim=0).to(self.device)
        self._clear_gpu()
        return prototype
    