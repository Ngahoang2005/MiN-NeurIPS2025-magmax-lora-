import math
import random
import numpy as np
from tqdm import tqdm
import torch
from torch import optim
from torch.nn import functional as F
from torch.utils.data import DataLoader
import copy
import gc 
import os

from utils.inc_net import MiNbaseNet
from torch.utils.data import WeightedRandomSampler
from utils.toolkit import tensor2numpy, count_parameters
from data_process.data_manger import DataManger
from utils.training_tool import get_optimizer, get_scheduler
from utils.toolkit import calculate_class_metrics, calculate_task_metrics

# Import Mixed Precision
from torch.amp import autocast, GradScaler

EPSILON = 1e-8

class MinNet(object):
    def __init__(self, args, loger):
        super().__init__()
        self.args = args
        self.logger = loger
        self._network = MiNbaseNet(args)
        self.device = args['device']
        self.num_workers = args["num_workers"]

        self.init_epochs = args["init_epochs"]
        self.init_lr = args["init_lr"]
        self.init_weight_decay = args["init_weight_decay"]
        self.init_batch_size = args["init_batch_size"]

        self.lr = args["lr"]
        self.batch_size = args["batch_size"]
        self.weight_decay = args["weight_decay"]
        self.epochs = args["epochs"]

        self.init_class = args["init_class"]
        self.increment = args["increment"]

        self.buffer_size = args["buffer_size"]
        self.buffer_batch = args["buffer_batch"]
        self.gamma = args['gamma']
        self.fit_epoch = args["fit_epochs"]

        self.known_class = 0
        self.cur_task = -1
        self.total_acc = []
        self.class_acc = []
        self.task_acc = []
        
        # Scaler cho Mixed Precision
        self.scaler = GradScaler('cuda')

    def _clear_gpu(self):
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def after_train(self, data_manger):
        if self.cur_task == 0:
            self.known_class = self.init_class
        else:
            self.known_class += self.increment

        _, test_list, _ = data_manger.get_task_list(self.cur_task)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)
        test_loader = DataLoader(test_set, batch_size=self.init_batch_size, shuffle=False,
                                 num_workers=self.num_workers)
        eval_res = self.eval_task(test_loader)
        self.total_acc.append(round(float(eval_res['all_class_accy']*100.), 2))
        self.logger.info('total acc: {}'.format(self.total_acc))
        self.logger.info('avg_acc: {:.2f}'.format(np.mean(self.total_acc)))
        self.logger.info('task_confusion_metrix:\n{}'.format(eval_res['task_confusion']))
        print('total acc: {}'.format(self.total_acc))
        print('avg_acc: {:.2f}'.format(np.mean(self.total_acc)))
        
        del test_set

    def save_check_point(self, path_name):
        torch.save(self._network.state_dict(), path_name)

    def compute_test_acc(self, test_loader):
        model = self._network.eval()
        correct, total = 0, 0
        device = self.device
        with torch.no_grad(), autocast('cuda'):
            for i, (_, inputs, targets) in enumerate(test_loader):
                inputs = inputs.to(device)
                outputs = model(inputs)
                logits = outputs["logits"]
                predicts = torch.max(logits, dim=1)[1]
                correct += (predicts.cpu() == targets).sum()
                total += len(targets)
        return np.around(tensor2numpy(correct) * 100 / total, decimals=2)

    @staticmethod
    def cat2order(targets, datamanger):
        for i in range(len(targets)):
            targets[i] = datamanger.map_cat2order(targets[i])
        return targets

    def init_train(self, data_manger):
        self.cur_task += 1
        train_list, test_list, train_list_name = data_manger.get_task_list(0)
        self.logger.info("task_list: {}".format(train_list_name))
        self.logger.info("task_order: {}".format(train_list))

        train_set = data_manger.get_task_data(source="train", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)

        train_loader = DataLoader(train_set, batch_size=self.init_batch_size, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.init_batch_size, shuffle=False,
                                 num_workers=self.num_workers)

        self.test_loader = test_loader

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = True

        self._network.update_fc(self.init_class)
        self._network.update_noise()
        
        self._clear_gpu()
        
        self.run(train_loader)
        self._network.after_task_magmax_merge()
        #self.analyze_model_sparsity()
        
        self._clear_gpu()
        
        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)
        self.fit_fc(train_loader, test_loader)

        train_set = data_manger.get_task_data(source="train_no_aug", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False

        self.re_fit(train_loader, test_loader)
        #self.check_rls_quality()
        del train_set, test_set
        self._clear_gpu()

    def increment_train(self, data_manger):
        self.cur_task += 1
        train_list, test_list, train_list_name = data_manger.get_task_list(self.cur_task)
        self.logger.info("task_list: {}".format(train_list_name))
        self.logger.info("task_order: {}".format(train_list))

        train_set = data_manger.get_task_data(source="train", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)
        test_set = data_manger.get_task_data(source="test", class_list=test_list)
        test_set.labels = self.cat2order(test_set.labels, data_manger)

        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                  num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                 num_workers=self.num_workers)

        self.test_loader = test_loader

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False

        self.fit_fc(train_loader, test_loader)

        self._network.update_fc(self.increment)

        train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True,
                                    num_workers=self.num_workers)
        self._network.update_noise()
        
        self._clear_gpu()

        self.run(train_loader)
        self._network.after_task_magmax_merge()
        #self.analyze_model_sparsity()
        
        self._clear_gpu()


        del train_set

        train_set = data_manger.get_task_data(source="train_no_aug", class_list=train_list)
        train_set.labels = self.cat2order(train_set.labels, data_manger)

        train_loader = DataLoader(train_set, batch_size=self.buffer_batch, shuffle=True,
                                    num_workers=self.num_workers)
        test_loader = DataLoader(test_set, batch_size=self.buffer_batch, shuffle=False,
                                    num_workers=self.num_workers)

        if self.args['pretrained']:
            for param in self._network.backbone.parameters():
                param.requires_grad = False

        self.re_fit(train_loader, test_loader)
        #self.check_rls_quality()
        del train_set, test_set
        self._clear_gpu()

    def fit_fc(self, train_loader, test_loader):
        self._network.eval()
        self._network.to(self.device)

        prog_bar = tqdm(range(self.fit_epoch))
        for _, epoch in enumerate(prog_bar):
            self._network.to(self.device)
            for i, (_, inputs, targets) in enumerate(train_loader):
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                targets = torch.nn.functional.one_hot(targets)
                self._network.fit(inputs, targets)
            
            info = "Task {} --> Update Analytical Classifier!".format(
                self.cur_task,
            )
            self.logger.info(info)
            prog_bar.set_description(info)
            self._clear_gpu()

    def re_fit(self, train_loader, test_loader):
        self._network.eval()
        self._network.to(self.device)
        prog_bar = tqdm(train_loader)
        for i, (_, inputs, targets) in enumerate(prog_bar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            targets = torch.nn.functional.one_hot(targets)
            self._network.fit(inputs, targets)

            info = "Task {} --> Reupdate Analytical Classifier!".format(
                self.cur_task,
            )
            
            self.logger.info(info)
            prog_bar.set_description(info)
        self._clear_gpu()

    def run(self, train_loader):
        scaler = GradScaler()

        if self.cur_task == 0:
            epochs = self.init_epochs
            lr = self.init_lr
            weight_decay = self.init_weight_decay
        else:
            epochs = self.epochs
            lr = self.lr
            weight_decay = self.weight_decay

        # Freeze toÃ n bá»™
        for p in self._network.parameters():
            p.requires_grad = False

        # Chá»‰ train noise cá»§a task hiá»‡n táº¡i + fc
        self._network.normal_fc.requires_grad_(True)
        for p in self._network.backbone.noise_maker[self.cur_task].parameters():
            p.requires_grad = True

        optimizer = get_optimizer(
            self.args["optimizer_type"],
            filter(lambda p: p.requires_grad, self._network.parameters()),
            lr,
            weight_decay,
        )

        scheduler = get_scheduler(self.args["scheduler_type"], optimizer, epochs)

        self._network.train().to(self.device)

        for epoch in range(epochs):
            total_loss, correct, total = 0, 0, 0

            for _, inputs, targets in train_loader:
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)

                noise = torch.randn(inputs.size(0), self.args["k"], device=self.device)

                optimizer.zero_grad(set_to_none=True)

                with autocast("cuda"):
                    if self.cur_task > 0:
                        # ======= CRITICAL FIX =======
                        with torch.no_grad():
                            out_old = self._network(
                                inputs, noise=noise, new_forward=False
                            )["logits"]

                        out_new = self._network(
                            inputs, noise=noise, new_forward=True
                        )["logits"]

                        logits = out_old + out_new
                    else:
                        logits = self._network(
                            inputs, noise=noise, new_forward=True
                        )["logits"]

                    loss = F.cross_entropy(logits, targets)

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()

                total_loss += loss.item()
                pred = logits.argmax(dim=1)
                correct += pred.eq(targets).sum().item()
                total += targets.size(0)

            scheduler.step()

            acc = 100. * correct / total
            self.logger.info(
                f"[Task {self.cur_task}] Epoch {epoch+1}/{epochs} "
                f"Loss {total_loss/len(train_loader):.4f} Acc {acc:.2f}"
            )
    def eval_task(self, test_loader):
        model = self._network.eval()
        pred, label = [], []
        with torch.no_grad():
            for i, (_, inputs, targets) in enumerate(test_loader):
                inputs = inputs.to(self.device)
                outputs = model(inputs)
                logits = outputs["logits"]
                predicts = torch.max(logits, dim=1)[1]
                pred.extend([int(predicts[i].cpu().numpy()) for i in range(predicts.shape[0])])
                label.extend(int(targets[i].cpu().numpy()) for i in range(targets.shape[0]))
        class_info = calculate_class_metrics(pred, label)
        task_info = calculate_task_metrics(pred, label, self.init_class, self.increment)
        return {
            "all_class_accy": class_info['all_accy'],
            "class_accy": class_info['class_accy'],
            "class_confusion": class_info['class_confusion_matrices'],
            "task_accy": task_info['all_accy'],
            "task_confusion": task_info['task_confusion_matrices'],
            "all_task_accy": task_info['task_accy'],
        }

    def get_task_prototype(self, model, train_loader):
        model = model.eval()
        model.to(self.device)
        features = []
        with torch.no_grad():
            for i, (_, inputs, targets) in enumerate(train_loader):
                inputs = inputs.to(self.device)
                with autocast('cuda'):
                    feature = model.extract_feature(inputs)
                features.append(feature.detach().cpu())
        
        all_features = torch.cat(features, dim=0)
        prototype = torch.mean(all_features, dim=0).to(self.device)
        self._clear_gpu()
        return prototype
    

    def analyze_model_sparsity(self, threshold=0):
        print("\n" + "="*50)
        print("ğŸ“Š PHÃ‚N TÃCH Äá»˜ THÆ¯A (SPARSITY REPORT)")
        print("="*50)

        # 1. Kiá»ƒm tra Analytic Classifier (RLS)
        w_rls = self._network.weight
        total_rls = w_rls.numel()
        
        # FIX: Chá»‰ tÃ­nh toÃ¡n khi ma tráº­n Ä‘Ã£ cÃ³ tham sá»‘ (sau Task 0 hoáº·c sau khi gá»i fit)
        if total_rls > 0:
            zero_rls = torch.sum(torch.abs(w_rls) <= threshold).item()
            sparsity_rls = (zero_rls / total_rls) * 100
            print(f"ğŸ”¹ Analytic Classifier (W_rls):")
            print(f"   - Tá»•ng tham sá»‘: {total_rls}")
            print(f"   - Äá»™ thÆ°a: {sparsity_rls:.2f}%")
        else:
            print(f"ğŸ”¹ Analytic Classifier (W_rls): ChÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o hoáº·c Ä‘ang trá»‘ng.")

        # 2. Kiá»ƒm tra cÃ¡c lá»›p PiNoise (Náº±m trong _network.backbone.noise_maker)
        print(f"\nğŸ”¹ PiNoise Modules (Backbone Layers):")
        total_mu_sparsity = []
        
        # Sá»­a lá»—i: Truy cáº­p qua self._network.backbone
        for i, noise_module in enumerate(self._network.backbone.noise_maker):
            # Kiá»ƒm tra mu.weight
            mu_w = noise_module.mu.weight.data
            zero_mu = torch.sum(torch.abs(mu_w) < threshold).item()
            sparsity_mu = (zero_mu / mu_w.numel()) * 100
            total_mu_sparsity.append(sparsity_mu)
            
            # In máº«u má»™t vÃ i layer Ä‘á»ƒ theo dÃµi
            if i % 4 == 0 or i == len(self._network.backbone.noise_maker) - 1:
                print(f"   - Layer {i:2d} | mu_weight sparsity: {sparsity_mu:.2f}%")

        avg_sparsity = np.mean(total_mu_sparsity)
        print("-" * 50)
        print(f"âœ… Trung bÃ¬nh Sparsity cá»§a Generator: {avg_sparsity:.2f}%")
        
        # Nháº­n xÃ©t dá»±a trÃªn ká»³ vá»ng cá»§a MagMax
        if avg_sparsity > 50:
            print("ğŸ’¡ Nháº­n xÃ©t: Tuyá»‡t vá»i! MagMax Ä‘ang giá»¯ cÃ¡c task khÃ¡ tÃ¡ch biá»‡t.")
        else:
            print("ğŸ’¡ Nháº­n xÃ©t: Äá»™ thÆ°a hÆ¡i tháº¥p. CÃ³ thá»ƒ cÃ¡c task Ä‘ang 'dáº«m chÃ¢n' nhau má»™t chÃºt.")
        print("="*50 + "\n")
    def check_rls_quality(self):
        """
        Script nhá» kiá»ƒm tra cháº¥t lÆ°á»£ng vÃ  Ä‘á»™ thÆ°a cá»§a ma tráº­n RLS
        """
        # Láº¥y trá»ng sá»‘ RLS
        rls_weight = model._network.weight.data  # [Buffer_size, Num_Classes]
        
        # 1. TÃ­nh Ä‘á»™ thÆ°a
        sparsity = (torch.abs(rls_weight) < 1e-6).float().mean().item() * 100
        
        # 2. TÃ­nh nÄƒng lÆ°á»£ng (Norm) - GiÃºp biáº¿t trá»ng sá»‘ cÃ³ bá»‹ 'ná»•' khÃ´ng
        weight_norm = torch.norm(rls_weight).item()
        
        # 3. Kiá»ƒm tra Ä‘á»™ lá»‡ch giá»¯a cÃ¡c class (Bias check)
        class_means = torch.mean(torch.abs(rls_weight), dim=0)
        class_std = torch.std(class_means).item()

        print(f"--- RLS Quality Check (Task {model.cur_task}) ---")
        print(f" > Sparsity: {sparsity:.2f}%")
        print(f" > Weight Norm: {weight_norm:.4f}")
        print(f" > Class Balance (Std of Means): {class_std:.6f}")
        
        if class_std > 0.1:
            print(" âš ï¸ Cáº£nh bÃ¡o: CÃ³ hiá»‡n tÆ°á»£ng lá»‡ch class (Recency Bias).")
        else:
            print(" âœ… RLS á»•n Ä‘á»‹nh: Trá»ng sá»‘ phÃ¢n bá»• Ä‘á»“ng Ä‘á»u giá»¯a cÃ¡c lá»›p.")
        print("-" * 35)